{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d67e3d1",
   "metadata": {},
   "source": [
    "# Forward mode automatic differentiation\n",
    "\n",
    "---\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "Automatic differentiation (AD) is a set of techniques to compute derivatives of functions in a computer program efficiently and accurately. Given a differentiable function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, AD seeks to compute the $m\\times n$ Jacobian matrix $J_{ij} = \\partial_{j}f_i$. Our convention here is that each column $j$ is the gradient vector $\\partial_{j} f$. If the domain is small and $f$ itself is computationally cheap, it may be efficient to compute each column separately. Denoting the computational cost of $f$ by $[f]$, this results in $\\mathcal{O}(n\\times[f])$ operations. Methods of this sort fall under the bracket of forward-mode AD (FWAD). When $n$ becomes large compared to $m$, it may be more efficient to consider other avenues. A popular method is backpropagation, where each row of the Jacobian (this is all $n$ derivatives corresponding to a single component of the function) is computed separately in a single pass. Methods such as these are called reverse-mode AD (RSAD), and carry a complexity of $\\mathcal{O}(m\\times[f])$.\n",
    "\n",
    "We focus here on FWAD, and in particular, its implementation through dual numbers. A dual number $d$ is an ordered pair of real numbers $(a, b)$, with $a$ the real component and $b$ the dual component. Its arithmetic is defined as\n",
    "\\begin{align*}\n",
    "&(a, b) + (c, d) = (a + c, b + d) \\\\\n",
    "&(a, b) \\times (c, d) = (ac, ad + bc)\n",
    "\\end{align*}\n",
    "Note the similarity with complex numbers $z$, which can also represented by an ordered pair of reals, $z = (a, b)$, and have an arithmetic of \n",
    "\\begin{align*}\n",
    "&(a, b) + (c, d) = (a + c, b + d) \\\\\n",
    "&(a, b) \\times (c, d) = (ac - bd, ad + bc)\n",
    "\\end{align*}\n",
    "The only difference is in the first term in the multiplication rule. By introducing the imaginary number $i$ that satisfies $i^2= - 1$, complex numbers can be represented as $z = a + bi$. A similar innovation can be applied to dual numbers, with $i$ replaced by the Grassmann number $\\epsilon$. $\\epsilon$ is nilpotent, with $\\epsilon^2 = 0$. Showing that this is consistent with the arithmetic rules is left to the reader as an exercise. We will denote by $\\mathbb{D}$ the space of dual numbers.\n",
    "\n",
    "Dual numbers allow us to compute exact derivatives of functions. For simplicity, we focus this discussion on $n, m = 1$. We will consider the general case at the end. Let $f:\\mathbb{R}\\to\\mathbb{R}$ be a smooth, real-valued function over the reals. We can extend it to be a dual-valued function over dual numbers by its Taylor series,\n",
    "\\begin{equation*}\n",
    "f(a + b\\epsilon) = \\sum_{i=0}^{n} \\frac{f^{(n)}(a)}{n!}(b\\epsilon)^n \\,,\n",
    "\\end{equation*}\n",
    "where now $f: \\mathbb{D}\\to\\mathbb{D}$. Since $\\epsilon^2 = 0$, this truncates to\n",
    "\\begin{equation*}\n",
    "f(a + b\\epsilon) = f(a) + f^{(1)}(a)b\\epsilon \\,.\n",
    "\\end{equation*}\n",
    "Setting $b = 1$, we find the dual part to be the derivative $f^{(1)}(a)$. By computing the extensions of functions to the duals, we are able to extract their (exact!) derivatives automatically. \n",
    "\n",
    "To generalise, define the $n$-dimensional dual space as $\\mathbb{D}^n = \\{a + b\\epsilon: a, b \\in \\mathbb{R}^n\\}$. We extend the smooth function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ to the duals through its Taylor expansion as before, and find\n",
    "\\begin{equation*}\n",
    "f(a + b\\epsilon) = f(a) + \\epsilon J(a)\\cdot b \\,,\n",
    "\\end{equation*}\n",
    "where $J(a)$ is the Jacobian evaluated at $a$. The dual part of this expression is the derivative of $f$ at $a$ along the direction of $b$. Running this calculation $n$ times, each taking $b$ to be a unit vector on $\\mathbb{R}^n$, then provides the full Jacobian matrix.\n",
    "\n",
    "It remains to implement dual numbers computationally. We will do this for the $n,m=1$ case for clarity. The steps are as follows:\n",
    "1) Define a dual number class that provides overrides of the standard arithmetic operators.\n",
    "2) Provide a wrapper for functions defined over real numbers to dual numbers.\n",
    "3) Implement the derivative operator as the dual component of a function $f$ evaluated at $d = a + \\epsilon$.\n",
    "\n",
    "For 1), we will present here the remaining arithmetic properties of dual numbers: division and exponentiation. Firstly, division. Let $a,b,c,d$ be generic real numbers, then\n",
    "\\begin{equation*}\n",
    "\\frac{a+b\\epsilon}{c+d\\epsilon} = \\frac{(a+b\\epsilon)(c-d\\epsilon)}{c^2} = \\frac{ac - (ad - bc)\\epsilon}{c^2} \\,.\n",
    "\\end{equation*}\n",
    "Note that this is only valid for $c \\neq 0$. If $c = 0$, the equation $a+b\\epsilon = ud\\epsilon$ is solvable for $u$ with $u=b/d$ provided $a=0$ and $d\\neq 0$. Technically, $b/d + v\\epsilon$ for any real number $v$ is a solution in this case: division by a dual number with 0 real part is not well-defined. In our implementation, we will restrict $v=0$. \n",
    "\n",
    "For exponentiation, let $c$ be a real number. We have, by Taylor expansion,\n",
    "\\begin{equation*}\n",
    "(a + b\\epsilon)^c = a^c + c b a^{c-1}\\epsilon \\,.\n",
    "\\end{equation*}\n",
    "We can extend this to exponentiation by dual numbers using the trick\n",
    "\\begin{equation*}\n",
    "(a + b\\epsilon)^{c + d\\epsilon} = \\exp((c + d\\epsilon)\\log (a+b\\epsilon)) \\,.\n",
    "\\end{equation*}\n",
    "By Taylor, \n",
    "\\begin{equation*}\n",
    "\\log(a + b\\epsilon) = \\log(a) + \\frac{b}{a}\\epsilon \\,,\n",
    "\\end{equation*}\n",
    "which requires $a > 0$. Appealing to Taylor again, we have\n",
    "\\begin{equation*}\n",
    "(a + b\\epsilon)^{c+d\\epsilon} = \\exp\\left(c\\log(a) +\\left(\\frac{bc}{a} + d\\log(a)\\right)\\epsilon\\right) = a^c\\left(1 +\\left(\\frac{bc}{a} + d\\log(a)\\right)\\epsilon \\right) \\,.\n",
    "\\end{equation*}\n",
    "We may extend this for $a < 0$ and consider complexified dual numbers, but this will not be done here for brevity.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4606040",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96f7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dual_numbers import autodiff, Dual, NUMERIC\n",
    "from dumpy import dp\n",
    "\n",
    "from theoria.validor import TestCase, Validor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98189520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples! we'll set out the examples in the following cells and collect them using the validator at the end\n",
    "\n",
    "# 1)\n",
    "# let's do a nested horror: f(x) = exp(1/x) + exp(-2 * sin(x)) - x^5 + x^x\n",
    "# this has: f'(x) = -exp(1/x)/x^2 - 2 * cos(x) * exp(-2 * sin(x)) - 5x^4 + x^x (1 + ln(x))\n",
    "\n",
    "def f1(x: Dual) -> Dual:\n",
    "    return dp.exp(1/x) + dp.exp(-2 * dp.sin(x)) - x ** 5 + x ** x\n",
    "\n",
    "def f1_expected_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return -np.exp(1/x) / x / x - 2 * np.cos(x) * np.exp(-2 * np.sin(x)) - 5 * x ** 4 + (x ** x) * (1 + np.log(x))\n",
    "\n",
    "def f1_auto_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return autodiff.diff(f1)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5d2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)\n",
    "# since we defined all arithmetic operations, this also works for functions defined with loops\n",
    "# for simplicity, let's present f(x) = 2x^4 as a while loop\n",
    "# we'll also look at its 2nd and 3rd derivatives, f''(x) = 24x^2 and f'''(x) = 48x\n",
    "\n",
    "def f2(x: Dual) -> Dual:\n",
    "    i = 0\n",
    "    while i < 2:\n",
    "        x *= x\n",
    "        i += 1\n",
    "    return 2 * x\n",
    "\n",
    "def f2_expected_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return 8 * x ** 3\n",
    "\n",
    "def f2_expected_second_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return 24 * x ** 2\n",
    "\n",
    "def f2_expected_third_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return 48 * x\n",
    "\n",
    "def f2_auto_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return autodiff.diff(f2)(x)\n",
    "\n",
    "def f2_auto_second_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return autodiff.diff(autodiff.diff(f2))(x)\n",
    "\n",
    "def f2_auto_third_derivative(x: NUMERIC) -> NUMERIC:\n",
    "    return autodiff.diff(autodiff.diff(autodiff.diff(f2)))(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ac2bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-04 09:32:33,314] [INFO] Test 1 RUNNING: actual -30.324559491950158 vs expected -30.324559491950158\n",
      "[2026-01-04 09:32:33,314] [INFO] Test 2 RUNNING: actual -10.606783408519464 vs expected -10.606783408519464\n",
      "[2026-01-04 09:32:33,314] [INFO] Test 3 RUNNING: actual -11.479106573910801 vs expected -11.479106573910803\n",
      "[2026-01-04 09:32:33,315] [INFO] Test 4 RUNNING: actual 223697.5752689876 vs expected 223697.5752689876\n",
      "[2026-01-04 09:32:33,315] [INFO] Test 5 RUNNING: actual 33025800934.910904 vs expected 33025800934.910904\n",
      "[2026-01-04 09:32:33,316] [INFO] All 5 tests passed for f1_auto_derivative.\n",
      "[2026-01-04 09:32:33,316] [INFO] Test 1 RUNNING: actual 1.0 vs expected 1.0\n",
      "[2026-01-04 09:32:33,316] [INFO] Test 2 RUNNING: actual 13.824 vs expected 13.823999999999998\n",
      "[2026-01-04 09:32:33,319] [INFO] Test 3 RUNNING: actual 14.886935999999999 vs expected 14.886935999999999\n",
      "[2026-01-04 09:32:33,323] [INFO] Test 4 RUNNING: actual 1906.6240000000003 vs expected 1906.6240000000003\n",
      "[2026-01-04 09:32:33,329] [INFO] Test 5 RUNNING: actual 8000.0 vs expected 8000.0\n",
      "[2026-01-04 09:32:33,333] [INFO] All 5 tests passed for f2_auto_derivative.\n",
      "[2026-01-04 09:32:33,335] [INFO] Test 1 RUNNING: actual 6.0 vs expected 6.0\n",
      "[2026-01-04 09:32:33,335] [INFO] Test 2 RUNNING: actual 34.56 vs expected 34.56\n",
      "[2026-01-04 09:32:33,336] [INFO] Test 3 RUNNING: actual 36.309599999999996 vs expected 36.309599999999996\n",
      "[2026-01-04 09:32:33,336] [INFO] Test 4 RUNNING: actual 922.5600000000002 vs expected 922.5600000000002\n",
      "[2026-01-04 09:32:33,336] [INFO] Test 5 RUNNING: actual 2400.0 vs expected 2400.0\n",
      "[2026-01-04 09:32:33,336] [INFO] All 5 tests passed for f2_auto_second_derivative.\n",
      "[2026-01-04 09:32:33,336] [INFO] Test 1 RUNNING: actual 24.0 vs expected 24.0\n",
      "[2026-01-04 09:32:33,337] [INFO] Test 2 RUNNING: actual 57.599999999999994 vs expected 57.599999999999994\n",
      "[2026-01-04 09:32:33,337] [INFO] Test 3 RUNNING: actual 59.04 vs expected 59.04\n",
      "[2026-01-04 09:32:33,337] [INFO] Test 4 RUNNING: actual 297.6 vs expected 297.6\n",
      "[2026-01-04 09:32:33,337] [INFO] Test 5 RUNNING: actual 480.0 vs expected 480.0\n",
      "[2026-01-04 09:32:33,337] [INFO] All 5 tests passed for f2_auto_third_derivative.\n"
     ]
    }
   ],
   "source": [
    "# generate tests\n",
    "\n",
    "x_vals = [0.5, 1.2, 1.23, 6.2, 10.0]\n",
    "expected_f1= [f1_expected_derivative(x) for x in x_vals]\n",
    "expected_f2 = [f2_expected_derivative(x) for x in x_vals]\n",
    "expected_f2_second = [f2_expected_second_derivative(x) for x in x_vals]\n",
    "expected_f2_third = [f2_expected_third_derivative(x) for x in x_vals]\n",
    "\n",
    "test_cases_f1 = [\n",
    "    TestCase(\n",
    "        input_data={\"x\": x},\n",
    "        expected_output=expected,\n",
    "        description=\"f(x) = exp(1/x) + exp(-2 * sin(x)) - x^5 + x^x: f'\",\n",
    "    )\n",
    "    for x, expected in zip(x_vals, expected_f1, strict=True)\n",
    "]\n",
    "\n",
    "test_cases_f2 = [\n",
    "    TestCase(\n",
    "        input_data={\"x\": x},\n",
    "        expected_output=expected,\n",
    "        description=\"f(x) = 2 * x^4 with loops: f'\",\n",
    "    )\n",
    "    for x, expected in zip(x_vals, expected_f2, strict=True)\n",
    "]\n",
    "\n",
    "test_cases_f2_second = [\n",
    "    TestCase(\n",
    "        input_data={\"x\": x},\n",
    "        expected_output=expected,\n",
    "        description=\"f(x) = 2 * x^4 with loops: f''\",\n",
    "    )\n",
    "    for x, expected in zip(x_vals, expected_f2_second, strict=True)\n",
    "]\n",
    "\n",
    "test_cases_f2_third = [\n",
    "    TestCase(\n",
    "        input_data={\"x\": x},\n",
    "        expected_output=expected,\n",
    "        description=\"f(x) = 2 * x^4 with loops: f'''\",\n",
    "    )\n",
    "    for x, expected in zip(x_vals, expected_f2_third, strict=True)\n",
    "]\n",
    "\n",
    "# 1e-8 is a reasonable tolerance\n",
    "def comparison(x: float, y: float) -> bool:\n",
    "    return np.isclose(x, y, atol=1e-8, rtol=1e-8)\n",
    "\n",
    "Validor(f1_auto_derivative).add_cases(test_cases_f1).run(comparison)\n",
    "Validor(f2_auto_derivative).add_cases(test_cases_f2).run(comparison)\n",
    "Validor(f2_auto_second_derivative).add_cases(test_cases_f2_second).run(comparison)\n",
    "Validor(f2_auto_third_derivative).add_cases(test_cases_f2_third).run(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57695b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theoria",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
